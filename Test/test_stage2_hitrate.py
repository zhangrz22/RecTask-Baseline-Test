#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3 Stage 2 æ¨èæ¨¡å‹çš„Hit Rateæµ‹è¯•è„šæœ¬
æµ‹è¯•Stage 2è®­ç»ƒåçš„æ¨èæ•ˆæœï¼Œå¹¶ä¸Stage 1è¿›è¡Œå¯¹æ¯”
"""
import argparse
import json
import os
import sys
import torch
import transformers
import torch.distributed as dist
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel
from peft import PeftModel
from torch.utils.data import DataLoader
from tqdm import tqdm
from data import SeqRecDataset
from utils import *
from collator import TestCollator
from evaluate import get_topk_results, get_metrics_results
import logging
import random
import datetime
import torch.distributed as dist
from torch.utils.data import DataLoader, DistributedSampler
from torch.nn.parallel import DistributedDataParallel
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np
from torch.utils.data import ConcatDataset
import pandas as pd

class Stage2ValDataset:
    """ç”¨äºStage 2éªŒè¯æ•°æ®çš„æ•°æ®é›†ç±»"""
    def __init__(self, val_data_path, sample_num=-1):
        # åŠ è½½éªŒè¯æ•°æ®
        self.df = pd.read_parquet(val_data_path)
        
        if sample_num > 0:
            self.df = self.df.head(sample_num)
        
        self.data = []
        for _, row in self.df.iterrows():
            # ä»instructionä¸­æå–å†å²åºåˆ—
            instruction = row['instruction']
            # ä»responseä¸­æå–ç›®æ ‡
            response = row['response']
            
            # åŒ¹é…TestCollatoræœŸæœ›çš„æ ¼å¼: input_ids, labels
            self.data.append({
                'input_ids': instruction + "\nResponse:",  # TestCollatorä¼šå¤„ç†è¿™ä¸ªæ ¼å¼
                'labels': response
            })
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]
    
    def get_all_items(self):
        # è¿”å›æ‰€æœ‰å¯èƒ½çš„itemsï¼ˆä»labelsä¸­æå–ï¼‰
        items = set()
        for item in self.data:
            items.add(item['labels'])
        return list(items)
    
    def get_prefix_allowed_tokens_fn(self, tokenizer):
        # ç®€åŒ–ç‰ˆæœ¬ï¼Œå…è®¸æ‰€æœ‰SID tokens
        def prefix_allowed_tokens(batch_id, input_ids):
            # å…è®¸æ‰€æœ‰ç‰¹æ®Štokens
            allowed_tokens = []
            vocab = tokenizer.get_vocab()
            for token, token_id in vocab.items():
                if token.startswith('<s_') or token in ['<|sid_begin|>', '<|sid_end|>']:
                    allowed_tokens.append(token_id)
            return allowed_tokens
        return prefix_allowed_tokens

def parse_args():
    parser = argparse.ArgumentParser(description="Qwen3 Stage 2 Hit Rate Test")
    
    # åŸºç¡€å‚æ•°
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--data_path", type=str, default="./", help="data directory")
    parser.add_argument("--dataset", type=str, default="Beauty", help="Dataset name")
    parser.add_argument("--index_file", type=str, default=".index.json", help="the item indices file")
    
    # æ¨¡å‹è·¯å¾„å‚æ•° - Stage 2 æ¨¡å‹
    parser.add_argument("--base_model_path", type=str, 
                        default="../Qwen3/model/Qwen3-1-7B-expanded-vocab",
                        help="Base model path (expanded vocab)")
    parser.add_argument("--stage2_model_path", type=str,
                        default="../Qwen3/results/stage2_recommendation_model", 
                        help="Stage 2 model path")
    
    # Stage 1æ¨¡å‹è·¯å¾„ï¼ˆStage 2éœ€è¦ï¼‰
    parser.add_argument("--stage1_model_path", type=str,
                        default="../Qwen3/results/sid_mapping_model", 
                        help="Stage 1 model path (needed for Stage 2 loading)")
    
    # Stage 2éªŒè¯æ•°æ®è·¯å¾„ï¼ˆå›ºå®šä½¿ç”¨ï¼‰
    parser.add_argument("--stage2_val_data_path", type=str,
                        default="../Qwen3/data_stage2/val.parquet", 
                        help="Path to Stage 2 validation data")
    
    # æ•°æ®ç›¸å…³å‚æ•°
    parser.add_argument("--max_his_len", type=int, default=20,
                        help="the max number of items in history sequence")
    parser.add_argument("--add_prefix", action="store_true", default=False,
                        help="whether add sequential prefix in history")
    parser.add_argument("--his_sep", type=str, default=", ", help="The separator used for history")
    
    # æµ‹è¯•å‚æ•°
    parser.add_argument("--test_batch_size", type=int, default=1)
    parser.add_argument("--num_beams", type=int, default=20)
    parser.add_argument("--sample_num", type=int, default=-1,
                        help="test sample number, -1 represents using all test data")
    parser.add_argument("--metrics", type=str, default="hit@1,hit@5,hit@10,ndcg@5,ndcg@10",
                        help="test metrics, separate by comma")
    parser.add_argument("--filter_items", action="store_true", default=False,
                        help="whether filter illegal items")
    
    # CoT/diagnostics
    parser.add_argument("--enable_cot", action="store_true", default=False,
                        help="enable two-stage generation: Think then constrained Response")
    parser.add_argument("--think_max_tokens", type=int, default=64,
                        help="max new tokens for the Think stage")
    parser.add_argument("--print_generations", action="store_true", default=False,
                        help="print prompts, think, and response candidates")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument("--log_file", type=str,
                        default="./logs/stage2_test.log",
                        help="all output log file path")
    
    return parser.parse_args()

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.enabled = False

def load_stage2_model(args, logger):
    """åŠ è½½Stage 2è®­ç»ƒå¥½çš„æ¨¡å‹ - æ­£ç¡®çš„åˆ†å±‚åŠ è½½"""
    logger.info("="*60)
    logger.info("ğŸ”„ Loading Qwen3 Stage 2 model...")
    logger.info("   Architecture: Base + Stage1(merged) + Stage2(LoRA)")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    try:
        # 1. åŠ è½½åˆ†è¯å™¨ï¼ˆä»Stage 2æ¨¡å‹ç›®å½•ï¼ŒåŒ…å«æ‰©å±•è¯æ±‡ï¼‰
        logger.info("ğŸ“ Loading tokenizer from Stage 2 model...")
        tokenizer = AutoTokenizer.from_pretrained(args.stage2_model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        # è®¾ç½®å·¦ä¾§paddingç”¨äºç”Ÿæˆä»»åŠ¡
        tokenizer.padding_side = "left"
        logger.info(f"âœ… Tokenizer loaded, vocab size: {len(tokenizer)}")
        
        # 2. åŠ è½½åŸºç¡€æ¨¡å‹ (æ‰©å±•è¯æ±‡è¡¨)
        logger.info("ğŸ—ï¸ Loading base model with expanded vocab...")
        base_model = AutoModelForCausalLM.from_pretrained(
            args.base_model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            trust_remote_code=True,
            device_map="auto"
        )
        logger.info("âœ… Base model loaded")
        
        # 3. åŠ è½½å¹¶åˆå¹¶Stage 1 LoRAæƒé‡ (SIDæ˜ å°„èƒ½åŠ›)
        logger.info("ğŸ”§ Loading and merging Stage 1 LoRA (SID mapping)...")
        stage1_model = PeftModel.from_pretrained(base_model, args.stage1_model_path)
        merged_model = stage1_model.merge_and_unload()
        logger.info("âœ… Stage 1 LoRA merged")
        
        # 4. åŠ è½½Stage 2 LoRAæƒé‡ (æ¨èå¢å¼ºèƒ½åŠ›)
        logger.info("ğŸ¯ Loading Stage 2 LoRA (recommendation enhancement)...")
        final_model = PeftModel.from_pretrained(merged_model, args.stage2_model_path)
        logger.info("âœ… Stage 2 LoRA loaded")
        
        # 5. å¯é€‰ï¼šåˆå¹¶Stage 2æƒé‡ä»¥æé«˜æ¨ç†é€Ÿåº¦
        logger.info("ğŸ”€ Merging Stage 2 weights for inference...")
        model = final_model.merge_and_unload()
        logger.info("âœ… All weights merged - ready for inference")
        
        model.eval()
        logger.info("="*60)
        
        return model, tokenizer
        
    except Exception as e:
        logger.error(f"âŒ Failed to load Stage 2 model: {e}")
        logger.error("æ¨¡å‹æ¶æ„è¯´æ˜:")
        logger.error("  Stage 2æ¨¡å‹ = Baseæ¨¡å‹ + Stage1(SIDæ˜ å°„) + Stage2(æ¨èå¢å¼º)")
        logger.error("å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        logger.error("1. æ£€æŸ¥base_model_pathæ˜¯å¦æ­£ç¡®")
        logger.error("2. æ£€æŸ¥stage1_model_pathæ˜¯å¦æ­£ç¡®") 
        logger.error("3. æ£€æŸ¥stage2_model_pathæ˜¯å¦æ­£ç¡®")
        logger.error("4. ç¡®ä¿Stage 2è®­ç»ƒå·²å®Œæˆ")
        raise

def load_stage1_model(args, logger):
    """åŠ è½½Stage 1æ¨¡å‹ç”¨äºå¯¹æ¯”"""
    logger.info("="*60)
    logger.info("ğŸ”„ Loading Qwen3 Stage 1 model for comparison...")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    try:
        # 1. åŠ è½½åˆ†è¯å™¨ï¼ˆä»Stage 1 LoRAæ¨¡å‹ç›®å½•ï¼‰
        logger.info("ğŸ“ Loading tokenizer from Stage 1 model...")
        tokenizer = AutoTokenizer.from_pretrained(args.stage1_model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        # è®¾ç½®å·¦ä¾§paddingç”¨äºç”Ÿæˆä»»åŠ¡
        tokenizer.padding_side = "left"
        logger.info(f"âœ… Tokenizer loaded, vocab size: {len(tokenizer)}")
        
        # 2. åŠ è½½åŸºç¡€æ¨¡å‹
        logger.info("ğŸ—ï¸ Loading base model...")
        base_model = AutoModelForCausalLM.from_pretrained(
            args.base_model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            trust_remote_code=True,
            device_map="auto"
        )
        logger.info("âœ… Base model loaded")
        
        # 3. åŠ è½½å¹¶åº”ç”¨LoRAæƒé‡
        logger.info("ğŸ”§ Loading LoRA adapter...")
        model = PeftModel.from_pretrained(base_model, args.stage1_model_path)
        logger.info("âœ… LoRA adapter loaded")
        
        # 4. åˆå¹¶LoRAæƒé‡ï¼ˆæé«˜æ¨ç†é€Ÿåº¦ï¼‰
        logger.info("ğŸ”€ Merging LoRA weights...")
        model = model.merge_and_unload()
        logger.info("âœ… Weights merged")
        
        model.eval()
        logger.info("="*60)
        
        return model, tokenizer
        
    except Exception as e:
        logger.error(f"âŒ Failed to load Stage 1 model: {e}")
        logger.error("å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        logger.error("1. æ£€æŸ¥Stage 1æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
        logger.error("2. æ£€æŸ¥åŸºç¡€æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
        raise

def test_tokenization(tokenizer, logger):
    """æµ‹è¯•SID tokençš„tokenizationæ˜¯å¦æ­£ç¡®"""
    logger.info("=== SID Tokenæµ‹è¯• ===")
    
    test_sid = "<|sid_begin|><s_a_156><s_b_218><s_c_251><s_d_244><|sid_end|>"
    
    # æ£€æŸ¥ç‰¹æ®Štokenæ˜¯å¦è¢«æ­£ç¡®è¯†åˆ«
    tokens = tokenizer.tokenize(test_sid)
    logger.info(f"SID tokenization: {tokens[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ªtoken
    
    # æ£€æŸ¥token ID
    token_ids = tokenizer.encode(test_sid, add_special_tokens=False)
    logger.info(f"Token IDs (first 5): {token_ids[:5]}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰UNK token
    unk_id = tokenizer.unk_token_id
    if unk_id in token_ids:
        logger.warning("âš ï¸  è­¦å‘Š: å‘ç°æœªçŸ¥token (UNK)ï¼ŒSID tokenå¯èƒ½æœªæ­£ç¡®åŠ è½½")
        return False
    else:
        logger.info("âœ… SID tokenè¯†åˆ«æ­£å¸¸")
        return True

def setup_logging(log_file):
    """è®¾ç½®è¯¦ç»†æ—¥å¿— - è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°"""
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    # åˆ›å»ºlogger
    logger = logging.getLogger('stage2_test')
    logger.setLevel(logging.DEBUG)
    
    # æ¸…é™¤å·²æœ‰handlers
    if logger.handlers:
        logger.handlers.clear()
    
    # æ–‡ä»¶handler
    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    
    # æ§åˆ¶å°handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # formatter
    formatter = logging.Formatter('[%(asctime)s] %(levelname)s: %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

def run_model_test(model, tokenizer, test_loader, all_items, prefix_allowed_tokens, 
                   args, logger, model_name):
    """è¿è¡Œå•ä¸ªæ¨¡å‹çš„æµ‹è¯•"""
    logger.info(f"ğŸš€ Starting {model_name} evaluation...")
    
    metrics = args.metrics.split(",")
    metrics_results = {}
    total = 0
    
    with torch.no_grad():
        for step, batch in enumerate(tqdm(test_loader, desc=f"Testing {model_name}")):
            inputs_texts = batch["inputs"]
            targets = batch["targets"]
            bs = len(targets)
            
            # === Stage 1: Think (optional) ===
            think_texts = [""] * bs
            if args.enable_cot and args.think_max_tokens > 0:
                logger.info(f"ğŸ¤” Starting CoT Think stage for batch {step}...")
                think_inputs_texts = [f"{msg}\nThink:" for msg in inputs_texts]
                
                # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€ä¸ªthink input
                if step < 3:
                    logger.info(f"Think input example: {think_inputs_texts[0][:200]}...")
                
                enc_think = tokenizer(
                    think_inputs_texts,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=tokenizer.model_max_length
                )
                enc_think = {k: v.to(model.device) for k, v in enc_think.items()}

                think_output = model.generate(
                    input_ids=enc_think["input_ids"],
                    attention_mask=enc_think.get("attention_mask", None),
                    max_new_tokens=args.think_max_tokens,
                    num_beams=1,
                    do_sample=False,
                    return_dict_in_generate=True,
                    output_scores=False,
                    early_stopping=True,
                )
                think_decoded = tokenizer.batch_decode(think_output["sequences"], skip_special_tokens=True)
                
                # è°ƒè¯•ï¼šæ‰“å°åŸå§‹thinkè¾“å‡º
                if step < 3:
                    logger.info(f"Raw think output example: {think_decoded[0][:300]}...")

                # æå–æ¯æ¡çš„ Think æ–‡æœ¬
                for i in range(bs):
                    full_text = think_decoded[i]
                    if "\nThink:" in full_text:
                        think_part = full_text.split("\nThink:")[-1]
                    else:
                        think_part = ""
                    if "Response:" in think_part:
                        think_part = think_part.split("Response:")[0]
                    think_texts[i] = think_part.strip()
                    
                    # è°ƒè¯•ï¼šè¾“å‡ºæå–çš„thinkæ–‡æœ¬
                    if step < 3:
                        logger.info(f"Extracted think text {i}: '{think_texts[i]}'")
                        
                logger.info(f"âœ… CoT Think stage completed for batch {step}")
            elif args.enable_cot:
                logger.info("âš ï¸  CoT enabled but think_max_tokens=0, skipping Think stage")

            # === Stage 2: Response (constrained) ===
            if args.enable_cot and args.think_max_tokens > 0:
                response_inputs_texts = [f"{msg}\nThink:{think_texts[i]}\nResponse:" for i, msg in enumerate(inputs_texts)]
                # è°ƒè¯•ï¼šæ˜¾ç¤ºåŒ…å«thinkçš„response input
                if step < 3:
                    logger.info(f"Response input with CoT: {response_inputs_texts[0][:300]}...")
            else:
                response_inputs_texts = [f"{msg}\nResponse:" for msg in inputs_texts]
            
            # ç¼–ç è¾“å…¥
            enc = tokenizer(
                response_inputs_texts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=tokenizer.model_max_length
            )
            enc = {k: v.to(model.device) for k, v in enc.items()}
            
            # beam searchç”Ÿæˆ
            num_beams = args.num_beams
            while True:
                try:
                    output = model.generate(
                        input_ids=enc["input_ids"],
                        attention_mask=enc.get("attention_mask", None),
                        max_new_tokens=10,
                        prefix_allowed_tokens_fn=prefix_allowed_tokens,
                        num_beams=num_beams,
                        num_return_sequences=num_beams,
                        output_scores=True,
                        return_dict_in_generate=True,
                        early_stopping=True,
                    )
                    break
                except RuntimeError as e:
                    err = str(e).lower()
                    if "out of memory" in err or "cuda" in err:
                        logger.warning(f"CUDA OOM with beam={num_beams}. Reducing beam size.")
                        num_beams -= 1
                        if num_beams < 1:
                            raise RuntimeError("Beam search OOM even with beam=1") from e
                        torch.cuda.empty_cache()
                    else:
                        raise
            
            # è§£ç è¾“å‡º
            output_ids = output["sequences"]
            scores = output.get("sequences_scores", None)
            decoded = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
            
            # è¯¦ç»†æ—¥å¿—è®°å½•å’Œç”Ÿæˆç»“æœæ‰“å°
            if args.print_generations:
                if scores is not None:
                    if hasattr(scores, 'detach'):
                        scores_list = [float(s) for s in scores.detach().cpu().tolist()]
                    else:
                        scores_list = [float(s) for s in scores]
                else:
                    scores_list = [float('nan')] * len(decoded)

                for i in range(bs):
                    start = i * num_beams
                    end = start + num_beams
                    cands = decoded[start:end]
                    cand_scores = scores_list[start:end]
                    
                    # ä½¿ç”¨infoçº§åˆ«ç¡®ä¿æ§åˆ¶å°ä¹Ÿèƒ½çœ‹åˆ°
                    logger.info(f"----- {model_name} SAMPLE {step*bs + i} -----")
                    logger.info(f"PROMPT:\n{inputs_texts[i]}")
                    if args.enable_cot and think_texts[i]:
                        logger.info(f"THINK:\n{think_texts[i]}")
                    logger.info("RESPONSE_CANDIDATES:")
                    for j, (c, sc) in enumerate(zip(cands, cand_scores)):
                        response = c.split("Response:")[-1].strip() if "Response:" in c else c
                        logger.info(f"  Rank {j+1}: score={sc:.4f} text={response}")
                    logger.info(f"TARGET: {targets[i]}")
                    
                    # æ·»åŠ åˆ†éš”çº¿ä¾¿äºé˜…è¯»
                    logger.info("-" * 50)
            
            # è®¡ç®—topkç»“æœ
            topk_res = get_topk_results(
                decoded, scores, targets, num_beams,
                all_items=all_items if args.filter_items else None
            )
            
            # ç´¯ç§¯metrics
            batch_metrics_res = get_metrics_results(topk_res, metrics)
            for m, res in batch_metrics_res.items():
                metrics_results[m] = metrics_results.get(m, 0.0) + res
            total += bs
            
            # ä¸­é—´è¿›åº¦æ±‡æŠ¥
            if (step + 1) % 50 == 0:
                temp_results = {m: metrics_results[m] / total for m in metrics_results}
                logger.info(f"[{model_name} Progress] Step {step+1}, Metrics: {temp_results}")
    
    # è®¡ç®—æœ€ç»ˆç»“æœ
    for m in metrics_results:
        metrics_results[m] = metrics_results[m] / total if total > 0 else 0.0
    
    return metrics_results

def run_stage2_test(args, logger=None):
    """è¿è¡ŒStage 2æµ‹è¯• - ä½¿ç”¨è®­ç»ƒæ—¶é¢„ç•™çš„éªŒè¯æ•°æ®"""
    set_seed(args.seed)
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥loggerï¼Œåˆ›å»ºä¸€ä¸ª
    if logger is None:
        logger = setup_logging(args.log_file)
    
    logger.info("ğŸ§ª Starting Qwen3 Stage 2 Hit Rate Test")
    logger.info(f"Args: {vars(args)}")
    
    # åŠ è½½Stage 2éªŒè¯æ•°æ®é›†ï¼ˆè®­ç»ƒæ—¶é¢„ç•™çš„æ•°æ®ï¼‰
    logger.info("ğŸ“Š Loading Stage 2 validation dataset...")
    logger.info(f"   Data source: {args.stage2_val_data_path}")
    logger.info("   Using Stage 2 validation data (preserved from training)")
    
    test_data = Stage2ValDataset(args.stage2_val_data_path, sample_num=args.sample_num)
    all_items = test_data.get_all_items()
    logger.info(f"ğŸ“ˆ Test data size: {len(test_data)}")
    
    # æµ‹è¯•å®Œæ•´çš„Stage 2æ¨¡å‹ (Base + Stage1 + Stage2)
    logger.info("\n" + "="*80)
    logger.info("ğŸ¯ Testing Complete Stage 2 Model")
    logger.info("    Architecture: Base + Stage1(SIDæ˜ å°„) + Stage2(æ¨èå¢å¼º)")
    logger.info("="*80)
    
    stage2_model, stage2_tokenizer = load_stage2_model(args, logger)
    
    # æµ‹è¯•tokenization
    if not test_tokenization(stage2_tokenizer, logger):
        logger.warning("Stage 2 SID tokenizationå¯èƒ½æœ‰é—®é¢˜ï¼Œä½†ç»§ç»­æµ‹è¯•...")
    
    stage2_collator = TestCollator(args, stage2_tokenizer)
    stage2_prefix_allowed_tokens = test_data.get_prefix_allowed_tokens_fn(stage2_tokenizer)
    
    stage2_loader = DataLoader(
        test_data,
        batch_size=args.test_batch_size,
        collate_fn=stage2_collator,
        shuffle=False,
        num_workers=2,
        pin_memory=True
    )
    
    results = run_model_test(
        stage2_model, stage2_tokenizer, stage2_loader, all_items, 
        stage2_prefix_allowed_tokens, args, logger, "Complete Stage 2"
    )
    
    # è¾“å‡ºæœ€ç»ˆç»“æœ
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š FINAL RESULTS")
    logger.info("="*80)
    
    logger.info("ğŸ¯ Complete Stage 2 Model Results:")
    for metric, value in results.items():
        logger.info(f"  {metric:>10}: {value:.4f}")
    
    logger.info("="*80)
    
    # è¾“å‡ºæµ‹è¯•æ‘˜è¦
    logger.info("\nğŸ“‹ Test Summary:")
    logger.info(f"Base Model: {args.base_model_path}")
    logger.info(f"Stage 1 Model: {args.stage1_model_path}")
    logger.info(f"Stage 2 Model: {args.stage2_model_path}")
    logger.info(f"Dataset: {args.dataset}")
    logger.info(f"Total samples: {len(test_data)}")
    logger.info(f"Batch size: {args.test_batch_size}")
    logger.info(f"Beam size: {args.num_beams}")
    logger.info(f"CoT enabled: {args.enable_cot}")
    if args.enable_cot:
        logger.info(f"Think max tokens: {args.think_max_tokens}")
    
    logger.info("\nâœ… Complete Stage 2 model testing completed successfully!")
    
    return results

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # è®¾ç½®æ—¥å¿—ï¼ˆå…ˆåˆ›å»ºä»¥ä¾¿è®°å½•é”™è¯¯ï¼‰
    logger = setup_logging(args.log_file)
    
    try:
        results = run_stage2_test(args, logger)
        logger.info("âœ… Stage 2 testing completed successfully!")
        return True
    except Exception as e:
        # å°†é”™è¯¯åŒæ—¶è¾“å‡ºåˆ°æ—¥å¿—å’Œæ§åˆ¶å°
        import traceback
        error_msg = f"âŒ Testing failed: {e}"
        logger.error(error_msg)
        logger.error("Traceback:")
        logger.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)