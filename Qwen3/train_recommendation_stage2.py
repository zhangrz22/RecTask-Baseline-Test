#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¬¬äºŒé˜¶æ®µæ¨èè®­ç»ƒè„šæœ¬
åœ¨ç¬¬ä¸€é˜¶æ®µSIDæ˜ å°„è®­ç»ƒåŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥åŠ å¼ºæ¨èèƒ½åŠ›
ä½¿ç”¨LoRAè®­ç»ƒæ›´å¤šå‚æ•°ï¼ˆQ,K,V,Oç­‰ï¼‰
"""

from transformers import (
    AutoConfig, AutoModelForCausalLM, AutoTokenizer, 
    Trainer, TrainingArguments, DataCollatorForLanguageModeling, 
    HfArgumentParser
)
import os
import torch
import json
import pandas as pd
from datasets import Dataset, load_dataset
import numpy as np
from dataclasses import dataclass, field
from typing import Optional
import random
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
import argparse

@dataclass
class ModelArguments:
    """
    æ¨¡å‹ç›¸å…³å‚æ•°
    """
    base_model_path: Optional[str] = field(
        default="../Qwen3/model/Qwen3-1-7B-expanded-vocab",
        metadata={"help": "ç¬¬ä¸€é˜¶æ®µåŸºç¡€æ¨¡å‹è·¯å¾„"}
    )
    stage1_lora_path: Optional[str] = field(
        default="../Qwen3/results/sid_mapping_model", 
        metadata={"help": "ç¬¬ä¸€é˜¶æ®µLoRAæ¨¡å‹è·¯å¾„"}
    )
    max_token_range: int = field(
        default=256,
        metadata={"help": "æ‰©å±•çš„ç‰¹æ®ŠtokenèŒƒå›´ï¼ˆs_a_0åˆ°s_a_255ç­‰ï¼‰"}
    )
    # LoRAå‚æ•°
    lora_r: int = field(default=64, metadata={"help": "LoRA rank"})
    lora_alpha: int = field(default=16, metadata={"help": "LoRA alpha"})
    lora_dropout: float = field(default=0.1, metadata={"help": "LoRA dropout"})
    lora_target_modules: str = field(
        default="q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj",
        metadata={"help": "LoRAç›®æ ‡æ¨¡å—ï¼Œé€—å·åˆ†éš”"}
    )

@dataclass 
class DataArguments:
    """
    æ•°æ®ç›¸å…³å‚æ•°
    """
    data_dir: str = field(
        default="./data_stage2",
        metadata={"help": "ç¬¬äºŒé˜¶æ®µè®­ç»ƒæ•°æ®ç›®å½•"}
    )
    max_seq_length: int = field(
        default=1024,
        metadata={"help": "æœ€å¤§åºåˆ—é•¿åº¦"}
    )
    train_file: Optional[str] = field(
        default="train.parquet",
        metadata={"help": "è®­ç»ƒæ•°æ®æ–‡ä»¶"}
    )
    validation_file: Optional[str] = field(
        default="val.parquet", 
        metadata={"help": "éªŒè¯æ•°æ®æ–‡ä»¶"}
    )

def load_dataset_from_file(file_path, data_format='parquet'):
    """
    åŠ è½½æ•°æ®é›†
    """
    if data_format == 'parquet':
        df = pd.read_parquet(file_path)
        return Dataset.from_pandas(df)
    elif data_format == 'json':
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return Dataset.from_list(data)
    else:
        raise ValueError(f"Unsupported format: {data_format}")

def get_extended_special_tokens(max_range=256):
    """
    è·å–æ‰©å±•çš„ç‰¹æ®Štokenåˆ—è¡¨ï¼ˆä¸ç¬¬ä¸€é˜¶æ®µä¿æŒä¸€è‡´ï¼‰
    
    Args:
        max_range: tokenèŒƒå›´ï¼ˆé»˜è®¤256ï¼‰
        
    Returns:
        list: ç‰¹æ®Štokenåˆ—è¡¨
    """
    special_tokens = []
    
    # æ·»åŠ æ§åˆ¶token
    special_tokens.extend(['<|sid_begin|>', '<|sid_end|>'])
    
    # æ·»åŠ s_*ç±»å‹çš„tokenï¼ˆ4ç»„Ã—256ï¼‰
    for prefix in ['s_a', 's_b', 's_c', 's_d']:
        for i in range(max_range):
            special_tokens.append(f'<{prefix}_{i}>')
    
    return special_tokens

def tokenize_function(examples, tokenizer, max_length=1024):
    """
    æ•°æ®tokenizationå‡½æ•°
    """
    tokenized = tokenizer(
        examples['text'],
        padding=False,
        truncation=True,
        max_length=max_length,
        return_tensors="pt" if len(examples['text']) == 1 else None,
        add_special_tokens=True,
        return_attention_mask=True,
    )
    
    # å¯¹äºå› æœè¯­è¨€æ¨¡å‹ï¼Œlabelsä¸input_idsç›¸åŒ
    tokenized['labels'] = tokenized['input_ids'].copy()
    
    return tokenized

def setup_model_and_tokenizer(model_args):
    """
    è®¾ç½®æ¨¡å‹å’Œtokenizer
    
    Args:
        model_args: æ¨¡å‹å‚æ•°
        
    Returns:
        tuple: (model, tokenizer)
    """
    print("="*60)
    print("ğŸ”„ Setting up Stage 2 model...")
    
    # 1. åŠ è½½tokenizerï¼ˆä»ç¬¬ä¸€é˜¶æ®µLoRAæ¨¡å‹è·¯å¾„ï¼ŒåŒ…å«æ‰©å±•çš„è¯æ±‡è¡¨ï¼‰
    print("ğŸ“ Loading tokenizer from Stage 1 LoRA model...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_args.stage1_lora_path)
        print(f"âœ… Tokenizer loaded from LoRA path: {model_args.stage1_lora_path}")
    except Exception as e:
        print(f"âš ï¸ Failed to load tokenizer from LoRA path: {e}")
        print("ğŸ“ Falling back to base model tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_args.base_model_path, use_fast=False)
        print(f"âœ… Tokenizer loaded from base model with slow tokenizer")
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print(f"âœ… Tokenizer loaded, vocab size: {len(tokenizer)}")
    
    # 2. åŠ è½½ç¬¬ä¸€é˜¶æ®µåŸºç¡€æ¨¡å‹
    print("ğŸ—ï¸ Loading stage 1 base model...")
    base_model = AutoModelForCausalLM.from_pretrained(
        model_args.base_model_path,
        torch_dtype=torch.float16,
        attn_implementation="flash_attention_2"
    )
    
    # 3. åŠ è½½ç¬¬ä¸€é˜¶æ®µLoRAæƒé‡
    print("ğŸ”§ Loading stage 1 LoRA weights...")
    stage1_model = PeftModel.from_pretrained(base_model, model_args.stage1_lora_path)
    
    # 4. åˆå¹¶ç¬¬ä¸€é˜¶æ®µæƒé‡
    print("ğŸ”€ Merging stage 1 weights...")
    model = stage1_model.merge_and_unload()
    
    print("âœ… Stage 1 model loaded and merged")
    
    # 5. å‡†å¤‡ç¬¬äºŒé˜¶æ®µLoRAé…ç½®
    print("âš™ï¸ Setting up stage 2 LoRA config...")
    
    # è·å–æ‰©å±•çš„ç‰¹æ®Štoken IDs
    special_tokens = get_extended_special_tokens(model_args.max_token_range)
    tokenized_special_tokens = tokenizer.convert_tokens_to_ids(special_tokens)
    
    # è¿‡æ»¤æ‰ä¸å­˜åœ¨çš„token
    valid_special_tokens = [tid for tid in tokenized_special_tokens if tid != tokenizer.unk_token_id]
    
    print(f"Special tokens: {len(special_tokens)} defined")
    print(f"Valid special token IDs: {len(valid_special_tokens)}")
    print(f"Example special token IDs: {valid_special_tokens[:10]}")
    
    # LoRAç›®æ ‡æ¨¡å—
    target_modules = model_args.lora_target_modules.split(',')
    target_modules = [mod.strip() for mod in target_modules]
    
    print(f"Target modules: {target_modules}")
    print(f"Embedding layer size: {model.get_input_embeddings().weight.shape[0]}")
    
    # é…ç½®LoRA
    lora_config = LoraConfig(
        r=model_args.lora_r,
        lora_alpha=model_args.lora_alpha,
        lora_dropout=model_args.lora_dropout,
        target_modules=target_modules,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        # æŒ‡å®šéœ€è¦è®­ç»ƒçš„ç‰¹æ®Štoken
        trainable_token_indices={
            'embed_tokens': valid_special_tokens,
        }
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
    trainable_params = 0
    all_params = 0
    for name, param in model.named_parameters():
        all_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
            if len(list(model.named_parameters())) < 20:  # åªåœ¨å‚æ•°ä¸å¤šæ—¶æ‰“å°
                print(f"Trainable: {name} - {param.shape}")
    
    print(f"\nğŸ“Š Parameter Summary:")
    print(f"  Trainable params: {trainable_params:,}")
    print(f"  All params: {all_params:,}")
    print(f"  Trainable %: {100 * trainable_params / all_params:.2f}%")
    
    print("="*60)
    
    return model, tokenizer

def main():
    # è§£æå‚æ•°
    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
    
    # è®¾ç½®label names
    training_args.label_names = ["labels"]
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(training_args.output_dir, exist_ok=True)
    os.makedirs(training_args.logging_dir, exist_ok=True)
    
    print(f"Output directory: {os.path.abspath(training_args.output_dir)}")
    print(f"Logging directory: {os.path.abspath(training_args.logging_dir)}")
    
    # è®¾ç½®æ¨¡å‹å’Œtokenizer
    model, tokenizer = setup_model_and_tokenizer(model_args)
    
    # åŠ è½½æ•°æ®é›†
    print("\nğŸ“Š Loading datasets...")
    
    train_file = os.path.join(data_args.data_dir, data_args.train_file)
    val_file = os.path.join(data_args.data_dir, data_args.validation_file)
    
    print(f"Loading training data from: {train_file}")
    train_dataset = load_dataset_from_file(train_file)
    
    print(f"Loading validation data from: {val_file}")
    eval_dataset = load_dataset_from_file(val_file)
    
    print(f"Train dataset size: {len(train_dataset)}")
    print(f"Eval dataset size: {len(eval_dataset)}")
    
    # æ•°æ®é¢„å¤„ç†
    print("ğŸ”„ Tokenizing datasets...")
    
    def tokenize_fn(examples):
        return tokenize_function(examples, tokenizer, data_args.max_seq_length)
    
    train_dataset = train_dataset.map(
        tokenize_fn,
        batched=True,
        remove_columns=train_dataset.column_names,
        desc="Tokenizing train dataset"
    )
    
    eval_dataset = eval_dataset.map(
        tokenize_fn,
        batched=True,
        remove_columns=eval_dataset.column_names,
        desc="Tokenizing eval dataset"
    )
    
    print("âœ… Tokenization completed")
    
    # æ•°æ®æ”¶é›†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # å› æœè¯­è¨€æ¨¡å‹
        pad_to_multiple_of=8,
    )
    
    # è®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("\nğŸš€ Starting Stage 2 Recommendation Training...")
    print(f"Training arguments: {training_args}")
    
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    print("ğŸ’¾ Saving final model...")
    trainer.save_model()
    tokenizer.save_pretrained(training_args.output_dir)
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    config_dict = {
        "stage": 2,
        "base_model_path": model_args.base_model_path,
        "stage1_lora_path": model_args.stage1_lora_path,
        "max_token_range": model_args.max_token_range,
        "lora_r": model_args.lora_r,
        "lora_alpha": model_args.lora_alpha,
        "lora_dropout": model_args.lora_dropout,
        "lora_target_modules": model_args.lora_target_modules,
        "max_seq_length": data_args.max_seq_length,
        "train_samples": len(train_dataset),
        "eval_samples": len(eval_dataset),
    }
    
    config_file = os.path.join(training_args.output_dir, "stage2_config.json")
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Stage 2 training completed!")
    print(f"Model saved to: {training_args.output_dir}")
    print(f"Config saved to: {config_file}")

if __name__ == "__main__":
    main()