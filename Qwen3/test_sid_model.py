#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•SIDæ˜ å°„æ¨¡å‹çš„æ¨ç†æ•ˆæœ
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import time

# --- é…ç½®åŒºåŸŸ ---
# åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆæ‰©å±•è¯æ±‡åçš„æ¨¡å‹ï¼‰
BASE_MODEL_PATH = "/home/liuzhanyu/Rec_baseline/RecTask-Baseline-Test/Qwen3/model/Qwen3-1-7B-expanded-vocab"
# LoRAæ¨¡å‹è·¯å¾„
LORA_MODEL_PATH = "/home/liuzhanyu/Rec_baseline/RecTask-Baseline-Test/Qwen3/results/sid_mapping_model"

# æµ‹è¯•ç”¨ä¾‹ï¼šåŒ¹é…æ–°çš„è®­ç»ƒæ ¼å¼ï¼ˆåŒ…å«thinkæ ‡ç­¾ï¼‰
TEST_CASES = [
    # é—®ç­”æ ¼å¼ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    "What is <|sid_begin|><s_a_156><s_b_218><s_c_251><s_d_244><|sid_end|>?",
    "Can you tell me what <|sid_begin|><s_a_156><s_b_218><s_c_251><s_d_244><|sid_end|> represents?",
    "Q: What does <|sid_begin|><s_a_156><s_b_218><s_c_251><s_d_244><|sid_end|> represent? A:",
    "Identify this product: <|sid_begin|><s_a_156><s_b_218><s_c_251><s_d_244><|sid_end|>",
]

# ä»å®é™…è®­ç»ƒæ•°æ®ä¸­è·å–çš„çœŸå®SID-æ ‡é¢˜å¯¹
GROUND_TRUTH_CASES = [
    {
        "sid": "<|sid_begin|><s_a_156><s_b_218><s_c_251><s_d_244><|sid_end|>",
        "expected_title": "MoYo Natural Labs 4 oz Travel Bottles, Empty Travel Containers with Flip Caps, BPA Free PET Plastic Squeezable Toiletry/Cosmetic Bottles (Neck 20-410) (Pack of 30, Clear)",
        "prompt": "<|sid_begin|><s_a_156><s_b_218><s_c_251><s_d_244><|sid_end|> represents"
    }
]

def test_tokenization(tokenizer):
    """æµ‹è¯•SID tokençš„tokenizationæ˜¯å¦æ­£ç¡®"""
    print("\n=== TokenåŒ–æµ‹è¯• ===")
    
    test_sid = "<|sid_begin|><s_a_156><s_b_218><s_c_251><s_d_244><|sid_end|>"
    
    # æ£€æŸ¥ç‰¹æ®Štokenæ˜¯å¦è¢«æ­£ç¡®è¯†åˆ«
    tokens = tokenizer.tokenize(test_sid)
    print(f"åŸå§‹SID: {test_sid}")
    print(f"TokenåŒ–ç»“æœ: {tokens}")
    
    # æ£€æŸ¥token ID
    token_ids = tokenizer.encode(test_sid, add_special_tokens=False)
    print(f"Token IDs: {token_ids}")
    
    # åå‘è§£ç 
    decoded = tokenizer.decode(token_ids, skip_special_tokens=False)
    print(f"åå‘è§£ç : {decoded}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰UNK token
    unk_id = tokenizer.unk_token_id
    if unk_id in token_ids:
        print("âš ï¸  è­¦å‘Š: å‘ç°æœªçŸ¥token (UNK)ï¼ŒSID tokenå¯èƒ½æœªæ­£ç¡®åŠ è½½")
    else:
        print("âœ… SID tokenè¯†åˆ«æ­£å¸¸")
    
    return len([t for t in tokens if '<s_' in t or '<|sid_' in t])

def load_model_and_tokenizer():
    """æ­£ç¡®åŠ è½½åŸºç¡€æ¨¡å‹å’ŒLoRAæƒé‡"""
    print("="*60)
    print("ğŸ”„ å¼€å§‹åŠ è½½æ¨¡å‹...")
    start_time = time.time()
    
    try:
        # 1. åŠ è½½åˆ†è¯å™¨ï¼ˆä»LoRAæ¨¡å‹ç›®å½•ï¼ŒåŒ…å«SID tokensï¼‰
        print("ğŸ“ åŠ è½½åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(LORA_MODEL_PATH)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        print(f"âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸï¼Œè¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
        
        # 2. åŠ è½½åŸºç¡€æ¨¡å‹
        print("ğŸ—ï¸  åŠ è½½åŸºç¡€æ¨¡å‹...")
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_PATH,
            torch_dtype=torch.float16,
            device_map="auto",
        )
        print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # 3. åŠ è½½å¹¶åº”ç”¨LoRAæƒé‡
        print("ğŸ”§ åŠ è½½LoRAé€‚é…å™¨...")
        model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)
        print("âœ… LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ")
        
        # 4. åˆå¹¶LoRAæƒé‡ï¼ˆå¯é€‰ï¼Œæé«˜æ¨ç†é€Ÿåº¦ï¼‰
        print("ğŸ”€ åˆå¹¶LoRAæƒé‡...")
        model = model.merge_and_unload()
        print("âœ… æƒé‡åˆå¹¶å®Œæˆ")
        
        load_time = time.time() - start_time
        print(f"â±ï¸  æ€»åŠ è½½æ—¶é—´: {load_time:.2f}ç§’")
        print("="*60)
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        print("\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("2. å®‰è£…PEFTåº“: pip install peft")
        print("3. ç¡®ä¿åŸºç¡€æ¨¡å‹å­˜åœ¨")
        raise

def test_inference(model, tokenizer, prompt, max_new_tokens=50):
    """æ‰§è¡Œå•æ¬¡æ¨ç†æµ‹è¯•"""
    print(f"\nğŸ“ æµ‹è¯•prompt: {prompt}")
    
    # ç¼–ç è¾“å…¥
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆå›ç­”
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,   # ä½¿ç”¨é‡‡æ ·é¿å…é‡å¤
            temperature=0.3,  # é™ä½æ¸©åº¦å‡å°‘éšæœºæ€§
            top_p=0.8,        # ä½¿ç”¨nucleusé‡‡æ ·
            repetition_penalty=1.2,  # é‡å¤æƒ©ç½š
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            early_stopping=True,  # é‡åˆ°EOSå°±åœæ­¢
        )
    
    # è§£ç è¾“å‡ºï¼ˆåªè·å–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
    new_tokens = outputs[0][inputs['input_ids'].shape[1]:]
    response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()
    
    print(f"ğŸ¤– æ¨¡å‹å›ç­”: {response}")
    return response

def test_ground_truth(model, tokenizer):
    """æµ‹è¯•è®­ç»ƒæ•°æ®ä¸­çš„çœŸå®æ˜ å°„"""
    print("\n" + "="*60)
    print("ğŸ¯ Ground Truthæµ‹è¯•ï¼ˆè®­ç»ƒæ•°æ®ä¸­çš„çœŸå®æ˜ å°„ï¼‰")
    
    for i, case in enumerate(GROUND_TRUTH_CASES, 1):
        print(f"\n--- Ground Truthæµ‹è¯• {i} ---")
        print(f"SID: {case['sid']}")
        print(f"é¢„æœŸæ ‡é¢˜: {case['expected_title'][:80]}...")
        
        response = test_inference(model, tokenizer, case['prompt'], max_new_tokens=80)
        
        # æ£€æŸ¥å…³é”®è¯åŒ¹é…
        expected_keywords = ['moyo', 'natural', 'labs', 'travel', 'bottles']
        found_keywords = [kw for kw in expected_keywords if kw in response.lower()]
        
        print(f"æ‰¾åˆ°å…³é”®è¯: {found_keywords}")
        
        if len(found_keywords) >= 3:
            print("âœ… åŒ¹é…è‰¯å¥½ï¼")
        elif len(found_keywords) >= 1:
            print("âš ï¸  éƒ¨åˆ†åŒ¹é…")
        else:
            print("âŒ åŒ¹é…å¤±è´¥")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    try:
        # 1. åŠ è½½æ¨¡å‹
        model, tokenizer = load_model_and_tokenizer()
        
        # 2. æµ‹è¯•tokenization
        num_sid_tokens = test_tokenization(tokenizer)
        print(f"âœ… è¯†åˆ«åˆ° {num_sid_tokens} ä¸ªSIDç›¸å…³token")
        
        # 3. Ground Truthæµ‹è¯•ï¼ˆä¼˜å…ˆï¼‰
        test_ground_truth(model, tokenizer)
        
        # 4. é€šç”¨æ¨ç†æµ‹è¯•
        print("\n" + "="*60)
        print("ğŸ§ª é€šç”¨æ¨ç†æµ‹è¯•...")
        
        for i, test_case in enumerate(TEST_CASES, 1):
            print(f"\n--- æµ‹è¯•ç”¨ä¾‹ {i} ---")
            response = test_inference(model, tokenizer, test_case)
            
            # ç®€å•æ£€æŸ¥æ˜¯å¦åŒ…å«å•†å“ç›¸å…³ä¿¡æ¯
            if any(keyword in response.lower() for keyword in ['travel', 'bottle', 'moyo', 'natural', 'labs']):
                print("âœ… å›ç­”ä¼¼ä¹åŒ…å«ç›¸å…³å•†å“ä¿¡æ¯")
            else:
                print("âš ï¸  å›ç­”å¯èƒ½ä¸å¤Ÿå‡†ç¡®")
        
        print("\n" + "="*60)
        print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
        
        # 5. æ€»ç»“å’Œå»ºè®®
        print("\nğŸ’¡ å¦‚æœç»“æœä¸ç†æƒ³ï¼Œå¯èƒ½çš„åŸå› :")
        print("1. è®­ç»ƒä¸å……åˆ† - éœ€è¦æ›´å¤šepochsæˆ–æ›´å¤§å­¦ä¹ ç‡")
        print("2. æ•°æ®è´¨é‡é—®é¢˜ - æ£€æŸ¥è®­ç»ƒæ•°æ®æ ¼å¼")
        print("3. ç”Ÿæˆå‚æ•°è®¾ç½® - è°ƒæ•´temperatureå’Œrepetition_penalty")
        print("4. æ¨¡å‹å®¹é‡é—®é¢˜ - å¯èƒ½éœ€è¦è®­ç»ƒæ›´å¤šå‚æ•°")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    return True

if __name__ == "__main__":
    success = main()